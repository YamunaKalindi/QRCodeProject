{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoVmd8lf+MSEB0uxDiLnJs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YamunaKalindi/QRCodeProject/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new, proper code starts here\n"
      ],
      "metadata": {
        "id": "uWTcpitw9JjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext faiss-cpu onnx tf2onnx dask\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv_fmR3S9H6V",
        "outputId": "6c9432c2-776f-451d-c055-be60fa665cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2024.11.2)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (25.2.10)\n",
            "Collecting protobuf>=3.20.2 (from onnx)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2024.10.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.6.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2025.1.31)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext faiss-cpu onnx tf2onnx dask\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K_N6kdbfj3-E",
        "outputId": "df853648-65c3-40b5-dc14-2a91adcab721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2024.11.2)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (25.2.10)\n",
            "Collecting protobuf>=3.20.2 (from onnx)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2024.10.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.6.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2025.1.31)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313475 sha256=08996ac8b5b79c600b8db89e24a088d3d53e16720dd3d5c3cef1ba11d0000986\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, protobuf, faiss-cpu, onnx, fasttext, tf2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed faiss-cpu-1.10.0 fasttext-0.9.3 onnx-1.17.0 protobuf-3.20.3 pybind11-2.13.6 tf2onnx-1.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "c14ef260263342f2aa9c1c2358267432"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "\n",
        "# Enable mixed precision for faster training\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# Load dataset using Dask\n",
        "print(\"🔹 Loading dataset...\")\n",
        "data_path = '/content/cleaned_dataset.csv'\n",
        "combined_df = dd.read_csv(data_path).compute()  # Convert to Pandas after loading\n",
        "print(f\"✅ Dataset loaded. Shape: {combined_df.shape}\")\n",
        "\n",
        "# Feature Extraction\n",
        "def calculate_entropy(url):\n",
        "    counts = Counter(url)\n",
        "    total_chars = len(url)\n",
        "    return -sum((count / total_chars) * np.log2(count / total_chars) for count in counts.values())\n",
        "\n",
        "def subdomain_count(url):\n",
        "    return urlparse(url).netloc.count('.')\n",
        "\n",
        "def digit_ratio(url):\n",
        "    return sum(c.isdigit() for c in url) / len(url)\n",
        "\n",
        "def special_char_count(url):\n",
        "    return sum(c in '@?&=' for c in url)\n",
        "\n",
        "def contains_ip(url):\n",
        "    return bool(re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url))\n",
        "\n",
        "print(\"🔹 Extracting features...\")\n",
        "combined_df['url_length'] = combined_df['URL'].apply(len)\n",
        "combined_df['entropy'] = combined_df['URL'].apply(calculate_entropy)\n",
        "combined_df['subdomain_count'] = combined_df['URL'].apply(subdomain_count)\n",
        "combined_df['digit_ratio'] = combined_df['URL'].apply(digit_ratio)\n",
        "combined_df['special_chars'] = combined_df['URL'].apply(special_char_count)\n",
        "combined_df['contains_ip'] = combined_df['URL'].apply(contains_ip).astype(int)\n",
        "\n",
        "print(f\"✅ Features extracted. Sample:\\n{combined_df[['url_length', 'entropy', 'subdomain_count', 'digit_ratio', 'special_chars', 'contains_ip']].head()}\")\n",
        "\n",
        "# Scale features\n",
        "X = combined_df[['url_length', 'entropy', 'subdomain_count', 'digit_ratio', 'special_chars', 'contains_ip']].values\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"✅ Features scaled.\")\n",
        "\n",
        "# Define target variable\n",
        "y = combined_df['label'].values\n",
        "\n",
        "# Train-test split\n",
        "print(\"🔹 Splitting dataset...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "print(f\"✅ Dataset split. Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
        "\n",
        "# Optimize data loading\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Build ANN Model with Batch Normalization & Dropout\n",
        "print(\"🔹 Building ANN model...\")\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, input_shape=(X_train.shape[1],), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(\"✅ Model compiled.\")\n",
        "\n",
        "# Train model with EarlyStopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "print(\"🔹 Training model...\")\n",
        "history = model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[early_stopping])\n",
        "print(\"✅ Training complete.\")\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"✅ Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu19zowq8Amf",
        "outputId": "1f348186-6ade-47cc-dea1-09c1e0ea8568"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Loading dataset...\n",
            "✅ Dataset loaded. Shape: (235795, 2)\n",
            "🔹 Extracting features...\n",
            "✅ Features extracted. Sample:\n",
            "   url_length   entropy  subdomain_count  digit_ratio  special_chars  \\\n",
            "0          32  3.929229                2          0.0              0   \n",
            "1          24  3.970176                2          0.0              0   \n",
            "2          30  4.164735                3          0.0              0   \n",
            "3          27  4.060262                2          0.0              0   \n",
            "4          34  3.917626                2          0.0              0   \n",
            "\n",
            "   contains_ip  \n",
            "0            0  \n",
            "1            0  \n",
            "2            0  \n",
            "3            0  \n",
            "4            0  \n",
            "✅ Features scaled.\n",
            "🔹 Splitting dataset...\n",
            "✅ Dataset split. Train size: (188636, 6), Test size: (47159, 6)\n",
            "🔹 Building ANN model...\n",
            "✅ Model compiled.\n",
            "🔹 Training model...\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.8057 - loss: 0.5366 - val_accuracy: 0.8200 - val_loss: 0.4315\n",
            "Epoch 2/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8166 - loss: 0.4352 - val_accuracy: 0.8216 - val_loss: 0.4267\n",
            "Epoch 3/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8174 - loss: 0.4323 - val_accuracy: 0.8213 - val_loss: 0.4234\n",
            "Epoch 4/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8171 - loss: 0.4302 - val_accuracy: 0.8224 - val_loss: 0.4215\n",
            "Epoch 5/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8173 - loss: 0.4294 - val_accuracy: 0.8223 - val_loss: 0.4202\n",
            "Epoch 6/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.8177 - loss: 0.4285 - val_accuracy: 0.8224 - val_loss: 0.4190\n",
            "Epoch 7/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8175 - loss: 0.4288 - val_accuracy: 0.8230 - val_loss: 0.4174\n",
            "Epoch 8/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8176 - loss: 0.4281 - val_accuracy: 0.8232 - val_loss: 0.4172\n",
            "Epoch 9/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8176 - loss: 0.4275 - val_accuracy: 0.8232 - val_loss: 0.4173\n",
            "Epoch 10/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8177 - loss: 0.4273 - val_accuracy: 0.8229 - val_loss: 0.4177\n",
            "Epoch 11/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8181 - loss: 0.4269 - val_accuracy: 0.8238 - val_loss: 0.4165\n",
            "Epoch 12/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8176 - loss: 0.4264 - val_accuracy: 0.8231 - val_loss: 0.4167\n",
            "Epoch 13/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8178 - loss: 0.4262 - val_accuracy: 0.8233 - val_loss: 0.4177\n",
            "Epoch 14/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8180 - loss: 0.4258 - val_accuracy: 0.8233 - val_loss: 0.4172\n",
            "Epoch 15/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8178 - loss: 0.4261 - val_accuracy: 0.8235 - val_loss: 0.4171\n",
            "Epoch 16/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8176 - loss: 0.4258 - val_accuracy: 0.8236 - val_loss: 0.4161\n",
            "Epoch 17/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8177 - loss: 0.4263 - val_accuracy: 0.8234 - val_loss: 0.4157\n",
            "Epoch 18/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.8185 - loss: 0.4255 - val_accuracy: 0.8238 - val_loss: 0.4165\n",
            "Epoch 19/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8190 - loss: 0.4255 - val_accuracy: 0.8233 - val_loss: 0.4157\n",
            "Epoch 20/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8184 - loss: 0.4253 - val_accuracy: 0.8231 - val_loss: 0.4171\n",
            "Epoch 21/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8177 - loss: 0.4254 - val_accuracy: 0.8234 - val_loss: 0.4172\n",
            "Epoch 22/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8184 - loss: 0.4252 - val_accuracy: 0.8240 - val_loss: 0.4153\n",
            "Epoch 23/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.8180 - loss: 0.4252 - val_accuracy: 0.8236 - val_loss: 0.4152\n",
            "Epoch 24/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8184 - loss: 0.4254 - val_accuracy: 0.8233 - val_loss: 0.4169\n",
            "Epoch 25/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8175 - loss: 0.4258 - val_accuracy: 0.8236 - val_loss: 0.4156\n",
            "Epoch 26/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8177 - loss: 0.4254 - val_accuracy: 0.8236 - val_loss: 0.4162\n",
            "Epoch 27/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8179 - loss: 0.4258 - val_accuracy: 0.8235 - val_loss: 0.4158\n",
            "Epoch 28/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8181 - loss: 0.4249 - val_accuracy: 0.8241 - val_loss: 0.4147\n",
            "Epoch 29/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.8177 - loss: 0.4250 - val_accuracy: 0.8236 - val_loss: 0.4152\n",
            "Epoch 30/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8174 - loss: 0.4249 - val_accuracy: 0.8234 - val_loss: 0.4148\n",
            "Epoch 31/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.8184 - loss: 0.4250 - val_accuracy: 0.8238 - val_loss: 0.4146\n",
            "Epoch 32/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8181 - loss: 0.4251 - val_accuracy: 0.8232 - val_loss: 0.4150\n",
            "Epoch 33/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8185 - loss: 0.4246 - val_accuracy: 0.8234 - val_loss: 0.4147\n",
            "Epoch 34/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8178 - loss: 0.4246 - val_accuracy: 0.8234 - val_loss: 0.4150\n",
            "Epoch 35/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.8183 - loss: 0.4244 - val_accuracy: 0.8236 - val_loss: 0.4155\n",
            "Epoch 36/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.8183 - loss: 0.4246 - val_accuracy: 0.8234 - val_loss: 0.4151\n",
            "✅ Training complete.\n",
            "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8238 - loss: 0.4151\n",
            "✅ Test Accuracy: 0.8238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_probs = model.predict(X_test)  # Get probabilities\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)  # Convert to binary labels\n",
        "\n",
        "# Print classification report\n",
        "print(\"🔹 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iaWlsBDPzkn",
        "outputId": "a5dd661c-1c93-408d-f31b-33dda7143e34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1474/1474\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
            "🔹 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9228    0.6406    0.7562     20124\n",
            "           1     0.7821    0.9601    0.8620     27035\n",
            "\n",
            "    accuracy                         0.8238     47159\n",
            "   macro avg     0.8525    0.8004    0.8091     47159\n",
            "weighted avg     0.8421    0.8238    0.8169     47159\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "\n",
        "# Enable mixed precision for faster training\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# Load dataset using Dask\n",
        "print(\"🔹 Loading dataset...\")\n",
        "data_path = '/content/cleaned_dataset.csv'\n",
        "combined_df = dd.read_csv(data_path).compute()  # Convert to Pandas after loading\n",
        "print(f\"✅ Dataset loaded. Shape: {combined_df.shape}\")\n",
        "\n",
        "# Feature Extraction\n",
        "def calculate_entropy(url):\n",
        "    counts = Counter(url)\n",
        "    total_chars = len(url)\n",
        "    return -sum((count / total_chars) * np.log2(count / total_chars) for count in counts.values())\n",
        "\n",
        "def subdomain_count(url):\n",
        "    return urlparse(url).netloc.count('.')\n",
        "\n",
        "def digit_ratio(url):\n",
        "    return sum(c.isdigit() for c in url) / len(url)\n",
        "\n",
        "def special_char_count(url):\n",
        "    return sum(c in '@?&=' for c in url)\n",
        "\n",
        "def contains_ip(url):\n",
        "    return bool(re.search(r'\\d+\\.\\d+\\.\\d+\\.\\d+', url))\n",
        "\n",
        "print(\"🔹 Extracting features...\")\n",
        "combined_df['url_length'] = combined_df['URL'].apply(len)\n",
        "combined_df['entropy'] = combined_df['URL'].apply(calculate_entropy)\n",
        "combined_df['subdomain_count'] = combined_df['URL'].apply(subdomain_count)\n",
        "combined_df['digit_ratio'] = combined_df['URL'].apply(digit_ratio)\n",
        "combined_df['special_chars'] = combined_df['URL'].apply(special_char_count)\n",
        "combined_df['contains_ip'] = combined_df['URL'].apply(contains_ip).astype(int)\n",
        "\n",
        "print(f\"✅ Features extracted. Sample:\\n{combined_df[['url_length', 'entropy', 'subdomain_count', 'digit_ratio', 'special_chars', 'contains_ip']].head()}\")\n",
        "\n",
        "# Scale features\n",
        "X = combined_df[['url_length', 'entropy', 'subdomain_count', 'digit_ratio', 'special_chars', 'contains_ip']].values\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"✅ Features scaled.\")\n",
        "\n",
        "# Define target variable\n",
        "y = combined_df['label'].values\n",
        "\n",
        "# Compute class weights to balance dataset\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(f\"🔹 Computed class weights: {class_weight_dict}\")\n",
        "\n",
        "# Train-test split\n",
        "print(\"🔹 Splitting dataset...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "print(f\"✅ Dataset split. Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
        "\n",
        "# Optimize data loading\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Build ANN Model with Batch Normalization & Dropout\n",
        "print(\"🔹 Building ANN model...\")\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(512, input_shape=(X_train.shape[1],), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "\n",
        "    # Additional Hidden Layer\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile model with class weighting\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(\"✅ Model compiled.\")\n",
        "\n",
        "# Train model with EarlyStopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "print(\"🔹 Training model...\")\n",
        "history = model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[early_stopping], class_weight=class_weight_dict)\n",
        "print(\"✅ Training complete.\")\n",
        "\n",
        "# Evaluate model\n",
        "y_pred_prob = model.predict(X_test).flatten()\n",
        "y_pred = (y_pred_prob > 0.7).astype(int)  # Adjust threshold to 0.7\n",
        "\n",
        "# Compute ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f\"✅ ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"🔹 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print(f\"✅ Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp5WMbTjTP6b",
        "outputId": "064c38dd-de54-4a36-9eb7-2323bcda4fc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Loading dataset...\n",
            "✅ Dataset loaded. Shape: (235795, 2)\n",
            "🔹 Extracting features...\n",
            "✅ Features extracted. Sample:\n",
            "   url_length   entropy  subdomain_count  digit_ratio  special_chars  \\\n",
            "0          32  3.929229                2          0.0              0   \n",
            "1          24  3.970176                2          0.0              0   \n",
            "2          30  4.164735                3          0.0              0   \n",
            "3          27  4.060262                2          0.0              0   \n",
            "4          34  3.917626                2          0.0              0   \n",
            "\n",
            "   contains_ip  \n",
            "0            0  \n",
            "1            0  \n",
            "2            0  \n",
            "3            0  \n",
            "4            0  \n",
            "✅ Features scaled.\n",
            "🔹 Computed class weights: {0: 1.1679379860319976, 1: 0.8742862439747868}\n",
            "🔹 Splitting dataset...\n",
            "✅ Dataset split. Train size: (188636, 6), Test size: (47159, 6)\n",
            "🔹 Building ANN model...\n",
            "✅ Model compiled.\n",
            "🔹 Training model...\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - accuracy: 0.7994 - loss: 0.6852 - val_accuracy: 0.8195 - val_loss: 0.4481\n",
            "Epoch 2/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8155 - loss: 0.4607 - val_accuracy: 0.8221 - val_loss: 0.4378\n",
            "Epoch 3/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8153 - loss: 0.4589 - val_accuracy: 0.8224 - val_loss: 0.4340\n",
            "Epoch 4/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - accuracy: 0.8155 - loss: 0.4562 - val_accuracy: 0.8221 - val_loss: 0.4373\n",
            "Epoch 5/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.8148 - loss: 0.4550 - val_accuracy: 0.8234 - val_loss: 0.4348\n",
            "Epoch 6/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8152 - loss: 0.4541 - val_accuracy: 0.8234 - val_loss: 0.4328\n",
            "Epoch 7/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.8156 - loss: 0.4537 - val_accuracy: 0.8227 - val_loss: 0.4355\n",
            "Epoch 8/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8159 - loss: 0.4524 - val_accuracy: 0.8228 - val_loss: 0.4315\n",
            "Epoch 9/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8161 - loss: 0.4516 - val_accuracy: 0.8230 - val_loss: 0.4301\n",
            "Epoch 10/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8159 - loss: 0.4507 - val_accuracy: 0.8232 - val_loss: 0.4288\n",
            "Epoch 11/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8163 - loss: 0.4495 - val_accuracy: 0.8221 - val_loss: 0.4296\n",
            "Epoch 12/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8160 - loss: 0.4501 - val_accuracy: 0.8220 - val_loss: 0.4299\n",
            "Epoch 13/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.8162 - loss: 0.4485 - val_accuracy: 0.8231 - val_loss: 0.4287\n",
            "Epoch 14/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.8154 - loss: 0.4493 - val_accuracy: 0.8222 - val_loss: 0.4296\n",
            "Epoch 15/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - accuracy: 0.8161 - loss: 0.4477 - val_accuracy: 0.8220 - val_loss: 0.4295\n",
            "Epoch 16/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8162 - loss: 0.4484 - val_accuracy: 0.8229 - val_loss: 0.4295\n",
            "Epoch 17/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8160 - loss: 0.4477 - val_accuracy: 0.8236 - val_loss: 0.4273\n",
            "Epoch 18/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - accuracy: 0.8164 - loss: 0.4480 - val_accuracy: 0.8230 - val_loss: 0.4283\n",
            "Epoch 19/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8160 - loss: 0.4472 - val_accuracy: 0.8229 - val_loss: 0.4305\n",
            "Epoch 20/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.8164 - loss: 0.4473 - val_accuracy: 0.8231 - val_loss: 0.4274\n",
            "Epoch 21/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.8168 - loss: 0.4462 - val_accuracy: 0.8228 - val_loss: 0.4277\n",
            "Epoch 22/50\n",
            "\u001b[1m2948/2948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.8163 - loss: 0.4463 - val_accuracy: 0.8227 - val_loss: 0.4276\n",
            "✅ Training complete.\n",
            "\u001b[1m1474/1474\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
            "✅ ROC-AUC Score: 0.8371\n",
            "🔹 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.71      0.74     20124\n",
            "           1       0.80      0.84      0.82     27035\n",
            "\n",
            "    accuracy                           0.79     47159\n",
            "   macro avg       0.78      0.78      0.78     47159\n",
            "weighted avg       0.78      0.79      0.78     47159\n",
            "\n",
            "\u001b[1m737/737\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8234 - loss: 0.4276\n",
            "✅ Test Accuracy: 0.8236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, \"qr_url_detector_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXukL6VuYwB-",
        "outputId": "c9262689-611c-47f8-e157-18a4e6db5f95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"qr_url_detector_model.keras\")\n"
      ],
      "metadata": {
        "id": "6hTmLfuGY4e2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load the model later, use this code snippet:\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"qr_url_detector_model.keras\")\n"
      ],
      "metadata": {
        "id": "fdWV4FERY8Zw"
      }
    }
  ]
}